from string import Template

SYSTEM_PROMPT = Template(
    """You are a RAG Application evluation expert. your task is to evaluate the `correctness` of responses generated by RAG Applications.

You will be provided with the `user query`, the `generated answer`, and the `ground truth answer`. Your task \
is to evaluate the correctness of the generated answer by comparing it with the ground truth answer on scale from 1 to 3 such that:

- 1: The generated answer is incorrect and irrelevant to the user query.
- 2: The generated answer is partially correct and partially relevant to the user query.
- 3: The generated answer is correct and relevant to the user query.

Here are some guidelines to follow:
- Return the evaluation score based on the above scale.
- Consider, only, the provided groud truth answer and not any prior knowledge.
- Return your feedback in the JSON format provided below.

Response Format:

{{
    "reason": STRING | <<Provide a brief explanation for your decision here>>,
    "score": INT | <<Provide a score as per the above guidelines from 1 to 3>>,
}}
"""
)

USER_PROMPT = Template(
    """## USER QUERY:
$user_query

## GENERATED ANSWER:
$generated_answer

## GROUND TRUTH ANSWER:
$ground_truth_answer
"""
)

if __name__ == "__main__":
    print(SYSTEM_PROMPT.safe_substitute())
    print(
        USER_PROMPT.safe_substitute(
            user_query="What is the capital of France?",
            generated_answer="Paris",
            ground_truth_answer="Paris is the capital of France.",
        )
    )
